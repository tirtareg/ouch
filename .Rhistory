# automatic resampling
resampling <- rsmp("cv", folds = 5L)
rr <- resample(task_iris, learner, resampling)
rr$score(measure)
rr$aggregate(measure)
knitr::opts_chunk$set(echo = TRUE)
library(mlr3)
# create learning task
task_iris <- TaskClassif$new(id = "iris", backend = iris, target = "Species")
task_iris
# load learner and set hyperparameter
learner <- lrn("classif.rpart", cp = 0.01)
learner
# train/test split
set.seed(100)
train_set <- sample(task_iris$nrow, 0.8 * task_iris$nrow)
test_set <- setdiff(seq_len(task_iris$nrow), train_set)
# train the model
learner$train(task_iris, row_ids = train_set)
# predict data
prediction <- learner$predict(task_iris, row_ids = test_set)
# calculate performance
prediction$confusion
measure <- msr("classif.acc")
prediction$score(measure)
# automatic resampling
resampling <- rsmp("cv", folds = 5L)
rr <- resample(task_iris, learner, resampling)
rr$score(measure)
rr$aggregate(measure)
knitr::opts_chunk$set(echo = TRUE)
library(mlr3)
# create learning task
task_iris <- TaskClassif$new(id = "iris", backend = iris, target = "Species")
task_iris
# load learner and set hyperparameter
learner <- lrn("classif.rpart", cp = 0.01)
learner
# train/test split
set.seed(100)
train_set <- sample(task_iris$nrow, 0.8 * task_iris$nrow)
test_set <- setdiff(seq_len(task_iris$nrow), train_set)
# train the model
learner$train(task_iris, row_ids = train_set)
# predict data
prediction <- learner$predict(task_iris, row_ids = test_set)
# calculate performance
prediction$confusion
measure <- msr("classif.acc")
prediction$score(measure)
# automatic resampling
resampling <- rsmp("cv", folds = 5L)
rr <- resample(task_iris, learner, resampling)
rr$score(measure)
rr$aggregate(measure)
knitr::opts_chunk$set(echo = TRUE)
library(mlr3)
# create learning task
task_iris <- TaskClassif$new(id = "iris", backend = iris, target = "Species")
task_iris
# load learner and set hyperparameter
learner <- lrn("classif.rpart", cp = 0.01)
learner
# train/test split
set.seed(100)
train_set <- sample(task_iris$nrow, 0.8 * task_iris$nrow)
test_set <- setdiff(seq_len(task_iris$nrow), train_set)
# train the model
learner$train(task_iris, row_ids = train_set)
# predict data
prediction <- learner$predict(task_iris, row_ids = test_set)
# calculate performance
prediction$confusion
measure <- msr("classif.acc")
prediction$score(measure)
# automatic resampling
resampling <- rsmp("cv", folds = 5L)
rr <- resample(task_iris, learner, resampling)
rr$score(measure)
rr$aggregate(measure)
knitr::opts_chunk$set(echo = TRUE)
library(mlr3)
# create learning task
task_iris <- TaskClassif$new(id = "iris", backend = iris, target = "Species")
task_iris
# load learner and set hyperparameter
learner <- lrn("classif.rpart", cp = 0.01)
learner
# train/test split
set.seed(100)
train_set <- sample(task_iris$nrow, 0.8 * task_iris$nrow)
test_set <- setdiff(seq_len(task_iris$nrow), train_set)
# train the model
learner$train(task_iris, row_ids = train_set)
# predict data
prediction <- learner$predict(task_iris, row_ids = test_set)
# calculate performance
prediction$confusion
measure <- msr("classif.acc")
prediction$score(measure)
# automatic resampling
resampling <- rsmp("cv", folds = 5L)
rr <- resample(task_iris, learner, resampling)
rr$score(measure)
rr$aggregate(measure)
knitr::opts_chunk$set(echo = TRUE)
library(mlr3)
# create learning task
task_iris <- TaskClassif$new(id = "iris", backend = iris, target = "Species")
task_iris
# load learner and set hyperparameter
learner <- lrn("classif.rpart", cp = 0.01)
learner
# train/test split
set.seed(100)
train_set <- sample(task_iris$nrow, 0.8 * task_iris$nrow)
test_set <- setdiff(seq_len(task_iris$nrow), train_set)
# train the model
learner$train(task_iris, row_ids = train_set)
# predict data
prediction <- learner$predict(task_iris, row_ids = test_set)
# calculate performance
prediction$confusion
measure <- msr("classif.acc")
prediction$score(measure)
# automatic resampling
resampling <- rsmp("cv", folds = 5L)
rr <- resample(task_iris, learner, resampling)
rr$score(measure)
rr$aggregate(measure)
knitr::opts_chunk$set(echo = TRUE)
library(mlr3)
# create learning task
task_iris <- TaskClassif$new(id = "iris", backend = iris, target = "Species")
task_iris
# load learner and set hyperparameter
learner <- lrn("classif.rpart", cp = 0.01)
learner
# train/test split
set.seed(100)
train_set <- sample(task_iris$nrow, 0.8 * task_iris$nrow)
test_set <- setdiff(seq_len(task_iris$nrow), train_set)
# train the model
learner$train(task_iris, row_ids = train_set)
# predict data
prediction <- learner$predict(task_iris, row_ids = test_set)
# calculate performance
prediction$confusion
measure <- msr("classif.acc")
prediction$score(measure)
# automatic resampling
resampling <- rsmp("cv", folds = 5L)
rr <- resample(task_iris, learner, resampling)
rr$score(measure)
rr$aggregate(measure)
knitr::opts_chunk$set(echo = TRUE)
library(mlr3)
# create learning task
task_iris <- TaskClassif$new(id = "iris", backend = iris, target = "Species")
task_iris
# load learner and set hyperparameter
learner <- lrn("classif.rpart", cp = 0.01)
learner
# train/test split
set.seed(100)
train_set <- sample(task_iris$nrow, 0.8 * task_iris$nrow)
test_set <- setdiff(seq_len(task_iris$nrow), train_set)
# train the model
learner$train(task_iris, row_ids = train_set)
# predict data
prediction <- learner$predict(task_iris, row_ids = test_set)
# calculate performance
prediction$confusion
measure <- msr("classif.acc")
prediction$score(measure)
# automatic resampling
resampling <- rsmp("cv", folds = 5L)
rr <- resample(task_iris, learner, resampling)
rr$score(measure)
rr$aggregate(measure)
knitr::opts_chunk$set(echo = TRUE)
library(mlr3)
# create learning task
task_iris <- TaskClassif$new(id = "iris", backend = iris, target = "Species")
task_iris
# load learner and set hyperparameter
learner <- lrn("classif.rpart", cp = 0.01)
learner
# train/test split
set.seed(100)
train_set <- sample(task_iris$nrow, 0.8 * task_iris$nrow)
test_set <- setdiff(seq_len(task_iris$nrow), train_set)
# train the model
learner$train(task_iris, row_ids = train_set)
# predict data
prediction <- learner$predict(task_iris, row_ids = test_set)
# calculate performance
prediction$confusion
measure <- msr("classif.acc")
prediction$score(measure)
# automatic resampling
resampling <- rsmp("cv", folds = 5L)
rr <- resample(task_iris, learner, resampling)
rr$score(measure)
rr$aggregate(measure)
knitr::opts_chunk$set(echo = TRUE)
library(mlr3)
# create learning task
task_iris <- TaskClassif$new(id = "iris", backend = iris, target = "Species")
task_iris
# load learner and set hyperparameter
learner <- lrn("classif.rpart", cp = 0.01)
learner
# train/test split
set.seed(100)
train_set <- sample(task_iris$nrow, 0.8 * task_iris$nrow)
test_set <- setdiff(seq_len(task_iris$nrow), train_set)
# train the model
learner$train(task_iris, row_ids = train_set)
# predict data
prediction <- learner$predict(task_iris, row_ids = test_set)
# calculate performance
prediction$confusion
measure <- msr("classif.acc")
prediction$score(measure)
# automatic resampling
resampling <- rsmp("cv", folds = 5L)
rr <- resample(task_iris, learner, resampling)
rr$score(measure)
rr$aggregate(measure)
data("mtcars", package = "datasets")
data = mtcars[, 1:3]
str(data)
library("mlr3")
task_mtcars = TaskRegr$new(id = "cars", backend = data, target = "mpg")
print(task_mtcars)
task_mtcars
library("mlr3viz")
install.packages('mlr3viz', dependencies = T)
library("mlr3viz")
install.packages('vctrs', dependencies = T)
library("mlr3viz")
autoplot(task_mtcars, type = "pairs")
autoplot(task_mtcars, type = "pairs")
a <- autoplot(task_mtcars, type = "pairs")
ggsave(a)
ggplot2::ggsave(a)
rlang::last_error()
library(Cairo)
install.packages('Cairo')
library(Cairo)
Cairo::Cairo(
30, #length
30, #width
file = paste("nameofplot", ".png", sep = ""),
type = "png", #tiff
bg = "transparent", #white or transparent depending on your requirement
dpi = 300,
units = "cm" #you can change to pixels etc
)
plot(a) #p is your graph object
tinytex::install_tinytex()
mlr_tasks
mlr_tasks$get('pima')
print(mlr_tasks$get('pima'))
mlr_tasks$get('pima')$data()
mlr_tasks$get('iris')$data()
class(mlr_tasks$get('iris')$data())
mlr_tasks
as.data.table(mlr_tasks)
task_mtcars$missings()
task_mtcars$missings()
task_mtcars$man
task_mtcars$man()
task_mtcars$missings()
task_iris = mlr_tasks$get("iris")
print(task_iris)
tsk('iris')
tsk('iris')
tsk('iris')$data()
task_iris$data()
task_iris$data()
tsk('iris')$data()
task_iris$data()
dim(tsk('iris')$data())
str(tsk('iris')$data())
summary(task_iris)
summary(task_iris$data())
summary(tsk('iris'))
tsk('iris')
summary(tsk('iris')$data())
reticulate::repl_python()
print('nmaku')
Y
update.packages(ask = F, checkBuilt = T)
install.packages('rlang')
install.packages("rlang")
library(discretization)
install.packages("discretization")
library(discretization)
data(iris)
mdlp(iris)$Disc.data
a <- mdlp(iris)
a$cutp
a$cutp[1]
str(iris)
a
summary(a)
a
a <- mdlp(iris)
a
str(a)
a
class(a)
a
a$Disc.data
summary(a$Disc.data)
mdlp(iris)$cutp
mdlp(iris)$cutp[1]
a <- rnorm(130, 10, 2)
mdlp(a)$cutp
dim(iris)
iris
mdlp(iris[,c(1, 5)])
mdlp(iris[,c(1, 5)])$cutp
mdlp(iris[,c(1, 5)])$cutp[1]
c(-Inf, mdlp(iris[,c(1,5)$cutp[1]]), Inf)
c(-Inf, mdlp(iris[,c(1,5])$cutp[1]]), Inf)
c(-Inf, mdlp(iris[,c(1,5])$cutp[1]), Inf)
c(-Inf, mdlp(iris[,c(1,5)])$cutp[1]), Inf)
c(-Inf, mdlp(iris[,c(1,5)])$cutp[1], Inf)
mdlp(iris[,c(1,5)])$cutp[[1]]
c(-Inf, mdlp(iris[,c(1,5)])$cutp[[1]], Inf)
data <- iris
data
library(dplyr)
data[data$Species == "Virginia",] <- "matamu"
data
data[data$Species == "virginica",] <- "matamu"
replace(data$Species, "virginica", "matamu")
library(plyr)
revalue(data$Species, c("virginica"="matamu"))
str(data)
data$Species <- as.character(data$Species)
replace(data$Species, "virginica", "matamu")
str(data)
recode_factor(data$Species, "virginica" = "matamu")
daata<-iris
recode_factor(data$Species, "virginica" = "matamu")
f <- as.factor(c("a", "b", "c"))
f
f <- as.factor(c("a", "b", "c", "a", "b", "c", "a", "b", "c"))
f
recode_factor(f, "a" = "x", "b" = "y")
f <- data$Species
f
data <- iris
f <- data$Species
recode_factor(f, "virginica" = "anu")
data <- iris
data$Species <- recode_factor(data$Species, "virginica" = "anu")
data
data$Species <- recode_factor(data$Species, "anu" = "matamu")
data
str(data)
data$Species <- as.character(data$Species)
data$Species <- recode_factor(data$Species, "matamu" = "anu")
data
str(data)
knitr::opts_chunk$set(echo = TRUE)
train <- read.csv("G:/My Drive/Dian/BRI HACKATON/train.csv")
test <- read.csv("G:/My Drive/Dian/BRI HACKATON/test.csv")
str(train)
summary(train)
dim(train)
dim(test)
dim(unique(train))
dim(unique(test))
summary(train)
unique(train$achievement_target_1)
names(train)
summary(train)
train <- mutate_if(train, is.character, as.factor)
library(tidyverse)
library(caret)
library(riv)
library(Information)
train <- mutate_if(train, is.character, as.factor)
test <- mutate_if(test, is.character, as.factor)
train <- train %>% mutate_all(na_if, "")
train <- droplevels(train)
test <- droplevels(test)
summary(train$year_graduated)
unique(train$year_graduated)
table(train$Best.Performance)
names(train)
train$Best.Performance <- factor(train$Best.Performance,
levels = c(0,1),
labels = c("not_reached", "reached"))
round(prop.table(table(train$Best.Performance))*100, 0)
a <- colSums(is.na(train))
b <- seq(1,length(a))
d <- round(colSums(is.na(train))/nrow(train)*100, 2)
c <- data.frame(peubah = b, banyaknya_NA = a, persen_NA = d)
arrange(c, desc(persen_NA))
train1 <- train[complete.cases(train), ]
test1 <- test
dim(train1)
table(train1$Best.Performance)
round(prop.table(table(train1$Best.Performance)) * 100, 0)
tipe <- NULL
nama <- NULL
for (i in (1 : (ncol(train1) - 1))) {
tipe[i] <- class(train1[, i])
nama[i] <- names(train1)[i]
}
data.frame(nama = nama, var = 1 : (ncol(train1) - 1), tipe = tipe)
# peubah numerik (2, 4, 5, 9, 11:13, 15:25)
sort(unique(train1[, 2]))  #numeric   job_duration_in_current_job_level
sort(unique(train1[, 4]))  #numeric   job_duration_in_current_person_level
sort(unique(train1[, 5]))  #numeric   job_duration_in_current_branch
sort(unique(train1[, 9]))  #numeric   age (kelompokkan)
sort(unique(train1[, 8])) #numeric   number_of_dependences (kelompokkan)
sort(unique(train1[, 10])) #numeric   number_of_dependences..male. (kelompokkan)
sort(unique(train1[, 12])) #numeric   number_of_dependences..female. (kelompokkan)
sort(unique(train1[, 10])) #numeric   number_of_dependences..male. (kelompokkan)
sort(unique(train1[, 12])) #numeric   number_of_dependences..female. (kelompokkan)
sort(unique(train1[, 13])) #numeric   GPA (margin too large)
sort(unique(train1[, 14])) #numeric   job_duration_as_permanent_worker (kelompokkan)
sort(unique(train1[, 15])) #numeric   job_duration_from_training (kelompokkan)
sort(unique(train1[, 16])) #numeric   branch_rotation (kelompokkan)
sort(unique(train1[, 17])) #numeric   job_rotation (kelompokkan)
sort(unique(train1[, 18])) #numeric   assign_of_otherposition (kelompokkan)
sort(unique(train1[, 19])) #numeric   annual.leave (kelompokkan)
sort(unique(train1[, 20])) #numeric   sick_leaves (kelompokkan)
sort(unique(train1[, 21])) #numeric   Avg_achievement_.
sort(unique(train1[, 22])) #numeric   Last_achievement_.
library(skimr)
skimmed <- skim(train1[, c(2, 4, 5, 9, 11, 12, 13, 15:24)])
library(skimr)
skimmed <- skim(train1[, c(2, 4, 5, 9, 11, 12, 13, 15)])
skimmed <- skimmed[, c(2,5,7,8,9,10,11)]
names(skimmed) <- c("Peubah","Mean","Min","Q1","Q2","Q3","Max")
skimmed
featurePlot(x = train1[, c(2, 4, 5, 15)],
y = train1[, 29],
plot = "box",
strip=strip.custom(par.strip.text=list(cex=.7)),
scales = list(x = list(relation="free"),
y = list(relation="free")))
featurePlot(x = train1[, c(2, 4, 5, 15)],
y = train1[, 22],
plot = "box",
strip=strip.custom(par.strip.text=list(cex=.7)),
scales = list(x = list(relation="free"),
y = list(relation="free")))
summary(train1)
ls()
### menjalankan algoritma SMOTE
library(DMwR)
set.seed(1000)
latih <- SMOTE(Best.Performance ~ ., data = train1, k = 5,
perc.over = 200, perc.under = 200)
# Feature Selection using Recursive Feature Elimination
ctrl <- rfeControl(functions = rfFuncs,
method = "cv",
verbose = FALSE)
lmProfile <- rfe(x=latih[, -ncol(latih)], y=latih$Best.Performance,
rfeControl = ctrl)
lmProfile
# Performing Random Forest
Control <- trainControl(method = "cv",
number = 5)
smote_rf <- train(Best.Performance ~ ., data = latih,
trCtrl = Control,
method = "rf")
smote_rf
# Performing Logistic Regression
Control <- trainControl(method = "cv",
number = 5)
smote_logit <- train(Best.Performance ~.,
data = latih,
method = "glmnet",
trControl = Control)
smote_logit
# Performing Logistic Regression
Control <- trainControl(method = "cv",
number = 5)
# Training neural net
smote_nn <- train(Best.Performance ~ .,
data = latih,
trControl = Control, method = "nnet")
smote_nn
reticulate::repl_python()
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from imblearn.pipeline import Pipeline
from sklearn.model_selection import cross_val_score
import warnings
warnings.filterwarnings('ignore')
quit()
exit()
exit
setwd("G:/My Drive/# REGISTRASI DAN STATISTIK/Website/webmaster/min_night")
setwd("G:/My Drive/# REGISTRASI DAN STATISTIK/Website/webmaster/min_night/ouch")
library(blogdown)
serve_site()
stop_server()
build_site()
